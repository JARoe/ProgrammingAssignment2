my_seq<-seq(5,10,length=30)
length(my_seq)
seq(1,length(my_seq))`
seq(1,length(my_seq))
))
end
()
my_seq
x
1:length(my_seq)
info()
1:length(my_seq)
seq(along.with=my_seq)
seq_along(my_seq)
rep(0,times=40)
rep(c(1,2,3),times=10)
rep(c(0,1,2,3),times=10)
rep(c(0,1,2),times=10)
rep(c(0,1,2),each=10)
num_vec <- c(0.5,55,-10,6)
num_vect <- c(0.5,55,-10,6)
tf<-num_vect<1
tf
num_vec >=6
num_vect >=6
my_car <- c("My","name","is")
my_char <- c("My","name","is")
my_char
paste(my_char,collapse = "")
paste(my_char, collapse = " ")
c(my_char,"Alisa")
my_name <- c(my_char,"Alisa")
my_name
paste(my_name,collapse = " ")
paste("Hello","world!",sep=" ")
paste(c(1:3),c("X","Y","Z"),sep="")
paste(LETTERS,1:4,sep="-")
c(44,NA,5,NA)
x<-c(44,NA,5,NA)
x*3
y<-rnorm(1000)
z<-rep(NA,1000)
my_data<-sample(c(y,z),100)
my_data
(c(y,z),100)
c(y,z),100)
c(y,z,100)
my_na<-is.na(my_data)
my_na
my_data==NA
sum(my_na)
my_data
0/0
Inf-Inf`
Inf-Inf
Inf-Inf
x
x[1:10]
x[is.na(x)]
y<-x(!is.na(x))
y<-x[!is.na(x)]
y
y[y>0]
x[x>0]
x[!is.na(x)&x>0]
x[3,5,7]
x[c(3,5,7)]
x[c(0)]
x[0]
x[3000]
x[c(-2,-10)]
x[-c(2,10)]
vect<-c(foo=11,bar=2,norf=NA)
vect
names(vect)
vect2<-c(11,2,NA)
names(vect2)<-c("foo","bar","norf")
identical(vect,vect2)
vect["bar"]
vect[c("foo","bar")]
my_vector<-1:20
my_vector
dim(my_vector)
length(my_vector)
dim(my_vector)<-c(4,5)
dim(my_vector)
attributes(my_vector)
my_vector
class(my_vector)
my_matrix<-my_vector
?matrix()
?matrix
my_matrix2<-matrix(1:20,4,5)
identical(my_matrix,my_matrix2)
patients<-c("Bill","Gina","Kelly","Sean")
cbind(patients,my_matrix)
my_data<-data.frame(patients,my_matrix)
my_data
class(my_data)
cnames<-c("patient","age","weight","bp","rating","test")
colname(my_data)
colnames(my_data)
cnames<-colnames(my_data)
colnames(my_data)<-cnames
my_data
getwd()
setwd(/Users/jule-alisaroessler/GoogleDrive/RProgramming)
setwd(Users/jule-alisaroessler/GoogleDrive/RProgramming)
my_data<-hq1_data.csv
my_data<-hw1_data.csv
?setwd
setwd("Users/jule-alisaroessler/GoogleDrive/RProgramming")
setwd("Users/jule-alisaroessler/downloads")
q()
n
q()
library(tidyverse)
library(data.table)
library(tidyverse)
library(data.table)
HelloData=data.table(letters, numbers)
numbers=1:25
HelloData=data.table(letters, numbers)
numbers
HelloData=data.table(c(letters, numbers))
HelloData
HelloData=data.table(letter, numbers)
letter=c('A','A','B','B','C','C','D','E','F','G','H','I','I','I','I','L','K','A','A','B','D','W','E','Q','Q')
numbers=1:25
HelloData=data.table(letter, numbers)
HelloData
HelloData$low= HelloData[, ifelse(number<10, 0,1)]
HelloData$low= HelloData[, ifelse(numbers<10, 0,1)]
HelloData
HelloData[,low2=case_when(numbers<5~2, numbers <10 ~1, TRUE ~0 )]
equivalence=data.table(letters=LETTERS, numbers=1:length(LETTERS))
merge(x=HelloData, y=equivalence, by.x='letter', by.y='letters')
HelloData$letters_to_numbers=match(x=HelloData$letter, table=LETTERS)
mean(HelloData$numbers)
mean(HelloData$letters_to_numbers)
std(HelloData$numbers)
std(HelloData$letters_to_numbers)
mean(HelloData$numbers)
mean(HelloData$letters_to_numbers)
std(HelloData$numbers)
std(HelloData$letters_to_numbers)
sd(HelloData$numbers)
sd(HelloData$letters_to_numbers)
lm(HelloData$letter ~ HelloData$letters_to_numbers)
lm(etter ~ letters_to_numbers, data=HelloData)
lm(letter ~ letters_to_numbers, data=HelloData)
lm(numbers ~ letters_to_numbers, data=HelloData)
HelloData2 = merge(HelloData$low, HelloData$letters_to_numbers)
HelloData2
sales[, ifelse(AAPL<=20%AAPL>=40, 1,ifelse(AAPL>40,2,0))
sales[, ifelse(AAPL<=20%AAPL>=40, 1,ifelse(AAPL>40,2,0))]
sales[, ifelse(AAPL<=20&AAPL>=40, 1,ifelse(AAPL>40,2,0))]
sales=fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Intro to R/Session 1/market_data.csv')
sales[, ifelse(AAPL<=20&AAPL>=40, 1,ifelse(AAPL>40,2,0))]
sales[,new_column:=sum(AAPL,GOOGL)]
View(sales)
sales=fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Intro to R/Session 1/market_data.csv')
library(data.table)
sales=fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Intro to R/Session 1/market_data.csv')
sales[,new_column:=sum(AAPL,GOOGL)]
sales[, ifelse(AAPL<=20&AAPL>=40, 1,ifelse(AAPL>40,2,0))]
sales[, AAPL_level:=ifelse(AAPL<=20&AAPL>=40, 1,ifelse(AAPL>40,2,0))]
View(sales)
sales = read.csv("/Users/jule-alisaroessler/Downloads/sales_ESADE.csv")
setDT(sales)
library(data.table)
library(lsr)
library(ggplot2)
library(magrittr)
sales = read.csv("/Users/jule-alisaroessler/Downloads/sales_ESADE.csv")
setDT(sales)
dates_by_store <- sales[, .(num_dates = uniqueN(date)), by= .(store, dept)]
length(unique(sales$store))
# Equivalent
uniqueN(sales$date)
sales[, uniqueN(dept), by = store][order(store)]
sales = read.csv("/Users/jule-alisaroessler/Downloads/sales_ESADE.csv")
sales = fread("/Users/jule-alisaroessler/Downloads/sales_ESADE.csv")
length(unique(sales$store))
# Equivalent
uniqueN(sales$date)
sales[, uniqueN(dept), by = store][order(store)]
dates_by_store <- sales[, .(num_dates = uniqueN(date)), by= .(store, dept)]
hist(dates_by_store$num_dates)
dates_by_store[,min(num_dates)]
dates_by_store[,min(num_dates)]
features= cor(sales[,.(sales_size,lag_1_sales_size,lag_2_sales_size,lag_4_sales_size,
lag_52_sales_size, is_holiday,temp,fuel_price,cpi,
unemployment,super_bowl, labor_day, christmas,md)],use ="complete.obs")
boxplot(sales$size, main = "Size")
# That is related with "store sales"
sales_by_size = sales[, .(sales = mean(as.numeric(sales)), size = mean(size)), by = store][order(store)]
plot(sales_by_size$size, sales_by_size$sales, xlab = "Store size", ylab = "Sales", main = "Sales by store size")
plot(sales[1:1000]$unemployment, sales[1:1000]$sales, xlab= "CPI", ylab = "Sales", main = "Sales by CPI")
arcvi_descriptivos2(sales$cpi, sales$sales, 4)
# First we will estandarize sales by size, to make it more comparable
sales$sales_size = sales$sales / sales$size
sales$lag_1_sales_size = sales$lag_1_sales / sales$size
sales$lag_2_sales_size = sales$lag_2_sales / sales$size
sales$lag_4_sales_size = sales$lag_4_sales / sales$size
sales$lag_52_sales_size = sales$lag_52_sales / sales$size
summary(sales$sales_size)
regressionVars = c("is_holiday",'temp','fuel_price','md','cpi','unemployment',
'super_bowl','thanks_giving','labor_day','christmas','lag_1_sales_size',
'lag_2_sales_size','lag_4_sales_size','lag_52_sales_size','sales_size')
features= cor(sales[,.(sales_size,lag_1_sales_size,lag_2_sales_size,lag_4_sales_size,
lag_52_sales_size, is_holiday,temp,fuel_price,cpi,
unemployment,super_bowl, labor_day, christmas,md)],use ="complete.obs")
varsCorrelation = features[,1]
varsCorrelation = sort(varsCorrelation)
varsCorrelation
varsCorrelation = features[,1]
varsCorrelation
uniqueN(sales$date)
sales[, uniqueN(dept), by = store][order(store)]
dates_by_store <- sales[, .(num_dates = uniqueN(date)), by= .(store, dept)]
hist(dates_by_store$num_dates)
daysStoreDept = sales[, .(days = .N), by = .(store, dept)]
head(daysStoreDept)
missingStoreDept = daysStoreDept[days < 143]
nrow(missingStoreDept)
summary(missingStoreDept)
sales[, .(days = uniqueN(date)), by= .(store, dept)][days<143]
sales$code = paste(sales$store, sales$dept)
missingStoreDept$code = paste(missingStoreDept$store, missingStoreDept$dept)
sales = sales[!code %in% missingStoreDept$code,]
summary(sales)
sales[sales >600000]
boxplot(sales$size, main = "Size")
sales_by_size = sales[, .(sales = mean(as.numeric(sales)), size = mean(size)), by = store][order(store)]
plot(sales_by_size$size, sales_by_size$sales, xlab = "Store size", ylab = "Sales", main = "Sales by store size")
plot(sales[1:1000]$unemployment, sales[1:1000]$sales, xlab= "CPI", ylab = "Sales", main = "Sales by CPI")
regressionVars = c("is_holiday",'temp','fuel_price','md','cpi','unemployment',
'super_bowl','thanks_giving','labor_day','christmas','lag_1_sales_size',
'lag_2_sales_size','lag_4_sales_size','lag_52_sales_size','sales_size')
features= cor(sales[,.(sales_size,lag_1_sales_size,lag_2_sales_size,lag_4_sales_size,
lag_52_sales_size, is_holiday,temp,fuel_price,cpi,
unemployment,super_bowl, labor_day, christmas,md)],use ="complete.obs")
varsCorrelation = features[,1]
varsCorrelation = sort(varsCorrelation)
varsCorrelation
train = sales[complete.cases(sales_size,
lag_1_sales_size,
lag_2_sales_size,
lag_4_sales_size,
lag_52_sales_size,
temp,
unemployment,
md,
fuel_price,
is_holiday,
super_bowl),]
m0 = glm(sales_size ~ lag_1_sales_size + lag_2_sales_size + lag_4_sales_size +
lag_52_sales_size + temp + unemployment + md + fuel_price + is_holiday + super_bowl,
data = train)
summary(m0)
sales[,uniqueN(date)]
View(sales)
meanSalesDept = sales[store == 1, .(meanSales = mean(sales)), by = dept]
meanSalesDept = meanSalesDept[order(dept)]
barplot(meanSalesDept$meanSales, names.arg = meanSalesDept$dept)
summary(meanSalesDept$meanSales)
meanSalesDept = sales[store == 1, .(meanSales = mean(sales)), by = dept]
meanSalesDept = meanSalesDept[order(dept)]
View(meanSalesDept)
summary(sales[store == 1 & dept == 47, .(date, sales)])
sales_1_47 = sales[store == 1 & dept == 47, .(date, sales)]
summary(sales_1_47)
summary(sales[store == 1 & dept == 47, .(date, sales)])
sales_1_47
sales$ones = 1
daysStoreDept = sales[, .(days = .N), by = .(store, dept)]
sales[, uniqueN(dept), by = store][order(store)]
sales$ones = 1
daysStoreDept = sales[, .(days = .N), by = .(store, dept)]
m0 = glm(sales_size ~ lag_1_sales_size + lag_2_sales_size + lag_4_sales_size +
lag_52_sales_size + temp + unemployment + md + fuel_price + is_holiday + super_bowl,
data = train)
summary(m0)
m1 = glm(sales_size ~ 0 + lag_1_sales_size + lag_2_sales_size + lag_4_sales_size +
lag_52_sales_size + unemployment + md + fuel_price + is_holiday + super_bowl,
data = train)
summary(m1)
error = abs(train$sales_size - predict(m2))
m2 = glm(sales_size ~ lag_4_sales_size + lag_52_sales_size +
unemployment + md + fuel_price + is_holiday + super_bowl,
data = train)
train = sales[complete.cases(sales_size,
lag_1_sales_size,
lag_2_sales_size,
lag_4_sales_size,
lag_52_sales_size,
temp,
unemployment,
md,
fuel_price,
is_holiday,
super_bowl),]
error = abs(train$sales_size - predict(m2))
error
predict(m2)
m0 = glm(sales_size ~ lag_1_sales_size + lag_2_sales_size + lag_4_sales_size +
lag_52_sales_size + temp + unemployment + md + fuel_price + is_holiday + super_bowl,
data = train)
summary(m0)
a=fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Session 3/R TEST/RTest.csv')
library(data.table)
a=fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Session 3/R TEST/RTest.csv')
View(a)
a=fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Session 3/R TEST/RTest.csv', sep='.',dec=',')
library(data.table) # we load the data table library
mydata = read.csv('market_data.csv',sep=',',dec='.') # remember 1. The csv file you are loading needs to be in the same folder as your script and 2. Click on Session > set working directory > to source file location.
# If the separator was ";" instead of "," we would need to change it in the read.csv function
setDT(mydata) # this transforms mydata into data.table format
# 5. How do you check basic statistical values of the columns of a data.table, say the mean or the median? and the standard deviation? and the quantiles?----
# Examples:
mean(mydata$AAPL,na.rm=T) # is the mean of column AAPL (Apple values)
median(mydata$AAPL,na.rm=T) # is the median of column AAPL (Apple values)
sd(mydata$AAPL,na.rm=T) # gives the standard deviation of Apple values
summary(mydata$AAPL) # gives the key statistical values such as mean, median but also the different quantiles (also known as 25%, 50% and 75% percentiles)
# 6. How do you check what's the meaning of a function and how does it work in R-Studio, say "median" or "sqrt"?----
help(median)
help(sqrt)
# 7. How do you create a new column out of previous columns in a data.table?----
# Example:
mydata[,new_column:=AAPL^2] # constructs a new column with the square of Apple price values
# 8. How can you update the values of a given column, only for certain values of another column, that is, only for *certain rows*?----
# Exercise example: create a column called AAPL_level with values:
# 1 if AAPL value is between 20 and 40
# 2 if AAPL value is bigger than 40
# 0 otherwise
mydata[,AAPL_level:=0] # we create the column and set it to 0 for all rows
mydata[AAPL>20,AAPL_level:=1] # we update the value to 1 when AAPL>20
mydata[AAPL>40,AAPL_level:=2] # we update the value to 2 when AAPL>40
mydata[,AAPL_level:=ifelse(AAPL>40,2, ifelse(AAPL>20,1,0))]
mydata[,AAPL_level:=ifelse(AAPL>40,2, ifelse(AAPL>20,1,0))]
library(data.table) # we load the data table library
mydata = read.csv('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Intro to R/Session 1/market_data.csv',sep=',',dec='.') # remember 1. The csv file you are loading needs to be in the same folder as your script and 2. Click on Session > set working directory > to source file location.
# If the separator was ";" instead of "," we would need to change it in the read.csv function
setDT(mydata) # this transforms mydata into data.table format
# 5. How do you check basic statistical values of the columns of a data.table, say the mean or the median? and the standard deviation? and the quantiles?----
# Examples:
mean(mydata$AAPL,na.rm=T) # is the mean of column AAPL (Apple values)
median(mydata$AAPL,na.rm=T) # is the median of column AAPL (Apple values)
sd(mydata$AAPL,na.rm=T) # gives the standard deviation of Apple values
summary(mydata$AAPL) # gives the key statistical values such as mean, median but also the different quantiles (also known as 25%, 50% and 75% percentiles)
# 6. How do you check what's the meaning of a function and how does it work in R-Studio, say "median" or "sqrt"?----
help(median)
help(sqrt)
# 7. How do you create a new column out of previous columns in a data.table?----
# Example:
mydata[,new_column:=AAPL^2] # constructs a new column with the square of Apple price values
mydata[,AAPL_level:=ifelse(AAPL>40,2, ifelse(AAPL>20,1,0))]
View(mydata)
loans= fread('loan_data.csv', stringsAsFactors = FALSE)
cor(loans$loan_amnt, loans$annual_inc)
loans[,high_inc:=ifelse(annual_inc>80000, 1,0)]
library(data.table)
loans= fread('loan_data.csv', stringsAsFactors = FALSE)
loans= fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Session 2/loan_data.csv', stringsAsFactors = FALSE)
loans[,high_inc:=ifelse(annual_inc>80000, 1,0)]
a=fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Session 3/R TEST/RTest.csv', sep=';',dec=',')
head(a)
library(ggplot2)
library(pROC)
testdata=fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Session 3/R TEST/RTest.csv', sep=';',dec=',')
head(testdata)
summary(testdata)
sqrt(var(testdata$Income))
testdata[,Value:=RiskPremium-CostClaims]
View(testdata)
mean(testdata$Value)
summary(testdata)
testdata[,DistanceLevel:=ifelse(Distance<1.5,0,ifelse(Distance>7.5,2,1))]
View(testdata)
glm(Income ~Distance, Women,FertilyAge, data=testdata, family='gaussian')
glm(Income ~Distance, Women,FertilityAge, data=testdata, family='gaussian')
lm(Income ~Distance+ Women+FertilityAge, data=testdata)
lm(Value~Income +Distance+ Women+FertilityAge, data=testdata)
1.303e-02*1000
1.303e-02*1
1.303e-02*1000
v=log(c(1:30)^2)
v[10]
a=v[v>3]
a
a=v[v<3]
a
summary(testdata)
sqrt(var(testdata$Income))
sd(testdata$Income)
lm(Value~Income +Distance+ Women+FertilityAge, data=testdata)
1.195e-02*1
testdata[,Value:=RiskPremium-CostClaims]
mean(testdata$Value)
1.303e-02*1000
# Load a CSV file (with comma as a delimiter and point as decimal separator) containing loan data
library(data.table)
# install.packages("pROC")
library(pROC)
lending_data = fread(file = "loan_data.csv",
header = TRUE,
sep = ",",
stringsAsFactors = FALSE,
dec=".")
rm(list = ls())
library(data.table)
library(tidyverse)
library(pROC)
transactional_data <- fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Challenge II/data/transactional_data.csv')
machine_failures   <- fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Challenge II/data/machine_failures.csv')
rm(list = ls())
library(data.table)
library(tidyverse)
library(pROC)
transactional_data <- fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Challenge II/data/transactional_data.csv')
machine_failures   <- fread('/Users/jule-alisaroessler/Google Drive/ESADE/Term 2/Data Analytics R/Challenge II/data/machine_failures.csv')
transactional_data <- merge(transactional_data, machine_failures, all.x=TRUE)
transactional_data[is.na(failure), failure:=0]
transactional_data
transactional_data <- transactional_data[order(machine, timestamp)]
transactional_data[, last_vend:=c(NA, timestamp[-.N]), by=machine]
transactional_data
transactional_data[, deltahours:=difftime(timestamp, last_vend, units='hours')]
transactional_data
transactional_data[,working_days:=uniqueN(date), by=machine]
sum_products = count(transactional_data, machine)
machine_daily_average = merge(transactional_data, sum_products, by=c('machine'), all.x=T)
machine_daily_average[, daily_sales_machine := n/working_days, ]
transactional_data$daily_sales_machine = machine_daily_average$daily_sales_machine
transactional_data[, working_days:=NULL]
transactional_data <- transactional_data[complete.cases(transactional_data), ]
transactional_data
transactional_data$delta <- transactional_data[, .(delta=as.numeric(deltahours/(24/daily_sales_machine), units='hours'))]
transactional_data
set.seed(0)
idx <- sample(x=seq_len(nrow(transactional_data)), size=round(0.7*nrow(transactional_data), 0))
train <- transactional_data[idx, ]
test <- transactional_data[-idx, ]
m <- glm(failure ~ delta, train, family=binomial)
summary(m)
cat('Intercept :', m$coefficients['(Intercept)'])
cat('\ndelta     : ', m$coefficients['delta'])
transactional_data[idx, prediction:=predict(m, type='response')]
transactional_data[-idx, prediction:=predict(m, newdata=transactional_data[-idx, ], type='response')]
roc.train <- roc(transactional_data[idx, failure], transactional_data[idx, prediction], quiet=TRUE)
roc.test  <- roc(transactional_data[-idx, failure], transactional_data[-idx, prediction], quiet=TRUE)
auc(roc.train)
auc(roc.test)
curve(expr=1/(1+exp(-(m$coefficients['(Intercept)'] + m$coefficients['delta']*x))),
from=-10, to=30,
xlab='delta', ylab='probability')
transactional_data[, priority:=case_when(prediction>=0.8 ~ 'high-risk',
prediction>=0.6 ~ 'med-risk',
TRUE ~ 'low-risk')]
transactional_data
calculate.log_threshold <- function(probability, model) {
threshold <- -(log(1/probability-1) + model$coefficients[1])/m$coefficients[2]
return(unname(threshold))
}
high.risk <- calculate.log_threshold(0.8, m)
med.risk <- calculate.log_threshold(0.6, m)
cat('Threshold for high-risk alarm :', high.risk)
cat('\nThreshold for med-risk alarm  :', med.risk)
high.risk.dt <- transactional_data[priority=='high-risk', ]
med.risk.dt <- transactional_data[priority=='med-risk', ]
total.risk.dt <- transactional_data[!priority=='low-risk', ]
high.alarms <- nrow(high.risk.dt) / uniqueN(transactional_data$date)
med.alarms <- nrow(med.risk.dt) / uniqueN(transactional_data$date)
total.alarms <- nrow(total.risk.dt) / uniqueN(transactional_data$date)
cat('Number of high-risk alarms :', round(high.alarms, 1))
cat('\nNumber of med-risk alarms  :', round(med.alarms, 1), "(excludes proportion that will classify as high-risk)")
cat('\nTotal number of alarms     :', round(total.alarms, 1), "(equivalent to total medium alarms)")
high.false_alarms <- (nrow(high.risk.dt) - sum(high.risk.dt$failure)) / nrow(high.risk.dt)
med.false_alarms <- (nrow(med.risk.dt) - sum(med.risk.dt$failure)) / nrow(med.risk.dt)
total.false_alarms <- (nrow(total.risk.dt) - sum(total.risk.dt$failure)) / nrow(total.risk.dt)
cat('Percentage of high-risk false alarms :', round(high.false_alarms*100, 2), '%')
cat('\nPercentage of med-risk false alarms  :', round(med.false_alarms*100, 2), '%')
cat('\nTotal percentage of false alarms     :', round(total.false_alarms*100, 2), '%')
######### ASSUMPTIONS #########
FIX_ALARM.TIME <- 1.5
FIX_ALARM.COST <- 10
CURRENT.FALSE_ALARMS <- 2.2
MARGIN <- 1.7
current_system.cost <- transactional_data[, .(period=difftime(max(date), min(date), unit='days')), by=machine]
current_system.cost <- as.numeric(sum(current_system.cost$period/365*CURRENT.FALSE_ALARMS*FIX_ALARM.COST))
###############################
med_alarm.dt <- transactional_data[delta>med.risk, ]                                              # HINT 1
med_alarm.dt[, threshold_hours:=med.risk*(24/daily_sales_machine)]                                # HINT 2
med_alarm.dt[, delta_fixed:=(threshold_hours+FIX_ALARM.TIME)*(1/(24/daily_sales_machine))]        # HINT 3
med_alarm.dt[, won_sales:=failure*(delta-delta_fixed)]                                            # HINT 4
med.add_rev <- sum(med_alarm.dt$won_sales)*MARGIN                                                 # HINT 5
med.false_cost <- nrow(med_alarm.dt[failure==0])*FIX_ALARM.COST                                   # HINT 6
med.profit <- (med.add_rev-med.false_cost+current_system.cost)/(MARGIN*nrow(transactional_data))  # HINT 7
cat('Added Revenue                  : €', med.add_rev)
cat('\nSaved cost from current system : €', current_cost)
setwd("~/Documents/GitHub/ProgrammingAssignment2")
source('~/.active-rstudio-document', echo=TRUE)
mydf = read.csv(path2csv, stringsAsFactors = FALSE)
mydf = read.csv(path2csv, stringsAsFactors = FALSE)
mydf -> read.csv(path2csv, stringsAsFactors = FALSE)
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
